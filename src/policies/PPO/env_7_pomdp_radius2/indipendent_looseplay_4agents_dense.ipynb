{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3cd819",
   "metadata": {},
   "source": [
    "# **My custom env - dense reward**\n",
    "\n",
    "**Environment classe pasted here in order to make it accessible to all the workers of Ray.**\n",
    "\n",
    "It is equivalent to /myenv_6_dense_reward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125ef575",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_examination = \"looseplay_4agents_densereward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b94dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project dir found: /home/terra/Desktop/magistrale/distributed_ai_project\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path(os.getcwd()) \n",
    "pointed_dir = current_dir\n",
    "\n",
    "while True:\n",
    "    if pointed_dir.is_dir() and \"distributed_ai_project\" == str(pointed_dir).split(\"/\")[-1]:\n",
    "        project_dir = pointed_dir\n",
    "        print(f\"Project dir found: {project_dir}\")\n",
    "        break\n",
    "    \n",
    "    if pointed_dir == pointed_dir.parent:\n",
    "        print(\"Error while looking for 'distributed_project', project dir\")\n",
    "        break\n",
    "        \n",
    "    pointed_dir = pointed_dir.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a20e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from pettingzoo import ParallelEnv\n",
    "import pygame\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "SPRITES_DIR = f\"{project_dir}/sprites\"\n",
    "class MyGridWorld(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"custom_grid_v0\"}\n",
    "\n",
    "    def __init__(self, render_mode=None, n_agents = 2, grid_size=15, vision_radius = 2):\n",
    "        if grid_size<8:\n",
    "            print(\"Error, need to insert a grid size greater or equal to 8\")\n",
    "        self.grid_size = grid_size\n",
    "        self.render_mode = render_mode\n",
    "        self.vision_radius = vision_radius\n",
    "        ######## Agents, fixed components, and forbidden positions\n",
    "        self.possible_agents = [f\"agent{i}\" for i in range(1, n_agents+1)]\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.agent_colors = [\n",
    "            (100, 150, 255),  # Blue\n",
    "            (255, 150, 100),  # Orange\n",
    "            (150, 255, 150),  # Green\n",
    "            (255, 150, 255),  # Pink\n",
    "            (150, 255, 255),  # Cyan\n",
    "            (255, 255, 150),  # Yellow\n",
    "        ]\n",
    "\n",
    "        self.gate_open = False\n",
    "        self.fixed_components = {\n",
    "            \"button1\":  {\"pos\": np.array([self.grid_size//2+2, 1+3+2]), \"file\": \"button.png\"},\n",
    "            \"button2\":  {\"pos\": np.array([self.grid_size//2+2, 1+1]), \"file\": \"button.png\"},\n",
    "\n",
    "            \"gate_open\":  {\"pos\": np.array([self.grid_size//2, 1+3]), \"file\": \"gate_open.png\"},\n",
    "            \"gate_close\":  {\"pos\": np.array([self.grid_size//2, 1+3]), \"file\": \"gate_close.png\"}\n",
    "        }\n",
    "        self.button1_pos = self.fixed_components[\"button1\"][\"pos\"]\n",
    "        self.button2_pos = self.fixed_components[\"button2\"][\"pos\"]\n",
    "\n",
    "        self.gate_pos = self.fixed_components[\"gate_open\"][\"pos\"]\n",
    "        self.target_final_pos = self.gate_pos+[0, -1]\n",
    "\n",
    "        self.x_range = (1, self.grid_size-1-1)\n",
    "        self.y_range = (1+3+1, self.grid_size-1-1)\n",
    "        self.forbidden_position = {tuple(self.button1_pos)}\n",
    "\n",
    "        self.max_cycles = 100\n",
    "        self.current_cycles = 0\n",
    "        ######## Pygame graphic configuration\n",
    "        self.window_size = 810\n",
    "        self.cell_size = self.window_size // self.grid_size\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # Walls\n",
    "        self.grid_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.grid_map[0, :] = 1\n",
    "        self.grid_map[:, 0] = 1\n",
    "        self.grid_map[-1, :] = 1\n",
    "        self.grid_map[:, -1] = 1\n",
    "        self.grid_map[ :, 1+3] = 1\n",
    "\n",
    "        # Sprites\n",
    "        self.agent_sprites = {}\n",
    "        self.component_sprites = {}\n",
    "\n",
    "        ######## Action space and observation space\n",
    "        # Five possible actions in each grid: stay(0), up(1), down(2), left(3), right(4)\n",
    "        self.action_spaces = {a: spaces.Discrete(5) for a in self.possible_agents}\n",
    "        OBSERVATION_DIM = 2*(len(self.agents)-1)+3*2+1\n",
    "        self.SENTINEL = 2* self.vision_radius\n",
    "        self.observation_spaces = {\n",
    "            a: spaces.Box(low=-self.vision_radius, high=self.SENTINEL, shape=(OBSERVATION_DIM,), dtype=np.float32)\n",
    "            for a in self.possible_agents\n",
    "        }\n",
    "        self.state_spaces = {a: None for a in self.possible_agents}\n",
    "\n",
    "    def _visible(self, my_pos, target_pos):\n",
    "        \"\"\"(Manhattan distance).\"\"\"\n",
    "        delta = target_pos - my_pos\n",
    "        is_visible = np.linalg.norm(delta, ord=1) <= self.vision_radius\n",
    "        if is_visible:\n",
    "          return True, delta.astype(np.float32)\n",
    "        else:\n",
    "          return False, np.array([self.SENTINEL,self.SENTINEL], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def state(self):\n",
    "        observations = self.gather_observations()\n",
    "        global_state = []\n",
    "\n",
    "        for agent in self.possible_agents:\n",
    "            if agent in observations:\n",
    "                global_state.append(observations[agent])\n",
    "            else:\n",
    "                # Se l'agente è morto/uscito, usa zeri\n",
    "                dim = self.observation_spaces[agent].shape[0]\n",
    "                global_state.append(np.zeros(dim, dtype=np.float32))\n",
    "\n",
    "        return np.concatenate(global_state)\n",
    "\n",
    "    # Boilerplate PettingZoo\n",
    "    def observation_space(self, agent): return self.observation_spaces[agent]\n",
    "    def action_space(self, agent): return self.action_spaces[agent]\n",
    "\n",
    "    '''\n",
    "    Step in the environment\n",
    "    '''\n",
    "    def step(self, actions):\n",
    "        if not self.agents: return {}, {}, {}, {}, {}\n",
    "        self.current_cycles += 1\n",
    "\n",
    "        rewards = {a: 0 for a in self.agents}\n",
    "\n",
    "        terminations = {a: False for a in self.agents}\n",
    "        truncations = {a: False for a in self.agents}\n",
    "        infos = {a: {\"is_success\":False} for a in self.agents}\n",
    "\n",
    "        desired_positions = {}\n",
    "        button_pressed = False\n",
    "        agents_desire_gate = []\n",
    "        for agent, action in actions.items():\n",
    "            current_pos = self.agent_positions[agent].copy()\n",
    "            target_pos = current_pos.copy()\n",
    "\n",
    "            if action == 1: target_pos[1] -= 1\n",
    "            elif action == 2: target_pos[1] += 1\n",
    "            elif action == 3: target_pos[0] -= 1\n",
    "            elif action == 4: target_pos[0] += 1\n",
    "\n",
    "            # Monitor button, gate and walls\n",
    "            is_wall = self.grid_map[target_pos[0], target_pos[1]] == 1\n",
    "            is_gate = (target_pos == self.gate_pos).all()\n",
    "            is_button = (target_pos == self.button1_pos).all() or (target_pos == self.button2_pos).all()\n",
    "\n",
    "            if is_button:\n",
    "                button_pressed = True\n",
    "            if is_gate:\n",
    "                agents_desire_gate.append((agent, current_pos))\n",
    "\n",
    "            if is_wall and not is_gate:\n",
    "                desired_positions[agent] = current_pos\n",
    "            else:\n",
    "                desired_positions[agent] = target_pos\n",
    "\n",
    "        # If gate was open remove a wall, if gate was closed update the position of those who wanted to cross it\n",
    "        self.gate_open = button_pressed\n",
    "        pushed_agent = None\n",
    "\n",
    "        if self.gate_open:\n",
    "            self.grid_map[self.gate_pos[0], self.gate_pos[1]] = 0\n",
    "        else:\n",
    "            self.grid_map[self.gate_pos[0], self.gate_pos[1]] = 1\n",
    "\n",
    "            for a in agents_desire_gate:\n",
    "                agent = a[0]\n",
    "                current_pos = a[1]\n",
    "                desired_positions[agent] = current_pos      # if current pos is not the gate pos\n",
    "\n",
    "                if (current_pos==self.gate_pos).all():      # if current pos is gate pos, in this case agent needs to be pushed down\n",
    "                    gate_x, gate_y = self.gate_pos\n",
    "                    candidate_cells = [\n",
    "                        np.array([gate_x,     gate_y + 1]),\n",
    "                        np.array([gate_x - 1, gate_y + 1]),\n",
    "                        np.array([gate_x + 1, gate_y + 1]),\n",
    "                    ]\n",
    "\n",
    "                    for cell in candidate_cells:\n",
    "                        if not any((cell == p).all() for p in self.agent_positions.values()):\n",
    "                            desired_positions[agent] = cell\n",
    "                            pushed_agent = (agent, cell)\n",
    "                            break\n",
    "                    # Note: if there is not a free cell the agent stays in the gate pos (should not happen)\n",
    "\n",
    "        # Check for conflicts (more than one agents have the same desired position)\n",
    "        final_positions = self.agent_positions.copy()\n",
    "        target_counts = defaultdict(list)         # Key: tuple(x, y) of desired positions, Value: list of agents desiring it\n",
    "        for agent, pos in desired_positions.items():\n",
    "            pos_tuple = tuple(pos)\n",
    "            target_counts[pos_tuple].append(agent)\n",
    "\n",
    "        # Solve eventual conflicts\n",
    "        for pos_tuple, agents_at_target in target_counts.items():\n",
    "            # Case 1: no contended position\n",
    "            if len(agents_at_target) == 1:\n",
    "                agent = agents_at_target[0]\n",
    "                final_positions[agent] = desired_positions[agent]\n",
    "\n",
    "            # Case 2: contended position\n",
    "            else:\n",
    "                if pushed_agent and tuple(pushed_agent[1]) == pos_tuple:\n",
    "                    winning_agent = pushed_agent[0]\n",
    "                else:\n",
    "                    one_agent_already_here_not_moving = None\n",
    "                    for agent in agents_at_target:\n",
    "                        if tuple(self.agent_positions[agent]) == pos_tuple:\n",
    "                            one_agent_already_here_not_moving = agent\n",
    "                            break\n",
    "\n",
    "                    winning_agent = one_agent_already_here_not_moving if one_agent_already_here_not_moving else random.choice(agents_at_target)\n",
    "\n",
    "                final_positions[winning_agent] = desired_positions[winning_agent]\n",
    "                agents_at_target.remove(winning_agent)\n",
    "                for losing_agent in agents_at_target[:]:\n",
    "                    final_positions[losing_agent] = self.agent_positions[losing_agent]\n",
    "        self.agent_positions = final_positions\n",
    "\n",
    "        # Negative reward if an agent is on the bottom area and is not pressing the button\n",
    "        agents_upper_area = 0\n",
    "        for a, pos in self.agent_positions.items():\n",
    "            dist_target = np.linalg.norm(pos - self.target_final_pos)\n",
    "            if pos[1]<4:                                # Agent on upper area\n",
    "                rewards[a] = +0.5\n",
    "                agents_upper_area+=1\n",
    "\n",
    "                if (pos == self.button2_pos).all():\n",
    "                    rewards[a] +=0.3\n",
    "            else:\n",
    "                rewards[a] -= (dist_target * 0.05)      # Agent on bottom area\n",
    "\n",
    "                if (pos == self.button1_pos).all():     # Agent on button\n",
    "                    rewards[a] += 0.3\n",
    "\n",
    "        if agents_upper_area == len(self.agents):\n",
    "            rewards = {a: +100 for a in self.agents}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "            infos = {a: {\"is_success\":True} for a in self.agents}\n",
    "\n",
    "        if self.current_cycles >= self.max_cycles:\n",
    "            truncations= {a: True for a in self.agents}\n",
    "\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        final_obs = self.gather_observations()\n",
    "        self.agents = [a for a in self.agents if not terminations[a]]\n",
    "        return final_obs, rewards, terminations, truncations, infos\n",
    "\n",
    "    '''\n",
    "    Determines initial condition of the simulation\n",
    "    '''\n",
    "\n",
    "    def generate_valid_position(self):\n",
    "        while True:\n",
    "            x = np.random.randint(self.x_range[0], self.x_range[1])\n",
    "            y = np.random.randint(self.y_range[0], self.y_range[1])\n",
    "            new_position = tuple((x, y))\n",
    "\n",
    "            if new_position not in self.forbidden_position:\n",
    "                return new_position\n",
    "\n",
    "    def generate_set_initial_positions(self):\n",
    "        set_initial_positions = set()\n",
    "        while len(set_initial_positions)<len(self.possible_agents):\n",
    "            new_pos = self.generate_valid_position()\n",
    "            set_initial_positions.add(new_pos)\n",
    "        return set_initial_positions\n",
    "\n",
    "    def generate_test_values(self, agent_num):\n",
    "        if agent_num==1:\n",
    "            return[7, 9]\n",
    "        else:\n",
    "            return [9,13]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        set_initial_positions = list(self.generate_set_initial_positions())\n",
    "        self.current_cycles = 0\n",
    "\n",
    "        self.agent_positions = {}\n",
    "        for i, agent_id in enumerate(self.agents):\n",
    "            self.agent_positions[agent_id] = np.array(set_initial_positions[i])\n",
    "\n",
    "        # To visualize the reset position if \"human\" mode on\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return self.gather_observations(), {a: {} for a in self.agents}\n",
    "\n",
    "    '''\n",
    "    Observation: [my_cur_pos, (other_cur) x number of other agents, button_pos, gate_pos, gate_open (1|0)]    '''\n",
    "    def gather_observations(self):\n",
    "        gate_status_info = np.array([int(self.gate_open)], dtype=np.float32)\n",
    "        observations = {}\n",
    "        for observing_agent in self.agents:\n",
    "            obs_segments = []\n",
    "            observing_agent_pos = self.agent_positions[observing_agent]\n",
    "\n",
    "            for other_agent in self.agents:\n",
    "                if other_agent != observing_agent:\n",
    "                    is_visible, delta = self._visible(observing_agent_pos,self.agent_positions[other_agent])\n",
    "                    obs_segments.append(delta)\n",
    "\n",
    "            is_visible, delta = self._visible(observing_agent_pos,self.button1_pos)\n",
    "            obs_segments.append(delta)\n",
    "\n",
    "            is_visible, delta = self._visible(observing_agent_pos,self.button2_pos)\n",
    "            obs_segments.append(delta)\n",
    "\n",
    "            is_visible, delta = self._visible(observing_agent_pos,self.gate_pos)\n",
    "            obs_segments.append(delta)\n",
    "\n",
    "            obs_segments.append(gate_status_info if is_visible else np.array([-1], dtype=np.float32))\n",
    "            observations[observing_agent] = np.concatenate(obs_segments, dtype=np.float32)\n",
    "        return observations\n",
    "\n",
    "    '''\n",
    "    Functions for Sprite rendering and loading\n",
    "    '''\n",
    "    def _create_missing_sprite(self, text, bg_color, border_color = (0,0,0)):\n",
    "        error_sprite = pygame.Surface((self.cell_size, self.cell_size), pygame.SRCALPHA)\n",
    "        error_sprite.fill((0, 0, 0, 0))\n",
    "        center = (self.cell_size // 2, self.cell_size // 2)\n",
    "        radius = int(self.cell_size * 0.4)\n",
    "        pygame.draw.circle(error_sprite, bg_color, center, radius)\n",
    "        pygame.draw.circle(error_sprite, border_color, center, radius, 2)\n",
    "\n",
    "        # Add text label\n",
    "        try:\n",
    "            font_size = int(self.cell_size * 0.22)\n",
    "            font = pygame.font.Font(None, font_size)\n",
    "            text_color = (0, 0, 0) if sum(bg_color) > 300 else (255, 255, 255)\n",
    "            text_surface = font.render(text, True, text_color)\n",
    "            text_rect = text_surface.get_rect(center=center)\n",
    "            error_sprite.blit(text_surface, text_rect)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Error while writing text inside missing sprite {text}. A missing sprite without text label will be used for this simulation.\")\n",
    "            pass\n",
    "\n",
    "        return error_sprite\n",
    "\n",
    "    def _load_and_scale_sprite(self, filename, component_name):\n",
    "        path = os.path.join(SPRITES_DIR, filename)\n",
    "\n",
    "        try:\n",
    "            image = pygame.image.load(path).convert_alpha()\n",
    "            scaled_size = int(self.cell_size )\n",
    "            return pygame.transform.scale(image, (scaled_size, scaled_size))\n",
    "\n",
    "        except (FileNotFoundError, pygame.error) as e:\n",
    "            print(f\"WARNING! Sprite image {path} not found. Using default sprite for this simulation.\")\n",
    "\n",
    "            text = component_name.upper() if component_name else \"ERROR\"\n",
    "            bg_color = (100, 100, 255) if \"AGENT\" in text else (128,128,128) # Blue agent, Gray fixed components\n",
    "            return self._create_missing_sprite(text, bg_color)\n",
    "\n",
    "    def _create_vision_overlay(self, agent_pos, agent_idx):\n",
    "        \"\"\"Create a semi-transparent surface showing agent's vision radius.\"\"\"\n",
    "        overlay = pygame.Surface((self.window_size, self.window_size), pygame.SRCALPHA)\n",
    "\n",
    "        # Get agent color with transparency\n",
    "        color = self.agent_colors[agent_idx % len(self.agent_colors)]\n",
    "\n",
    "        # Draw vision cells with gradient effect\n",
    "        center_x, center_y = agent_pos[0], agent_pos[1]\n",
    "\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                # Calculate Manhattan distance\n",
    "                dist = abs(x - center_x) + abs(y - center_y)\n",
    "\n",
    "                if dist <= self.vision_radius:\n",
    "                    # Gradient alpha: stronger at center, weaker at edges\n",
    "                    alpha = int(60 * (1 - dist / (self.vision_radius + 1)))\n",
    "                    cell_color = (*color, alpha)\n",
    "\n",
    "                    rect = pygame.Rect(\n",
    "                        x * self.cell_size,\n",
    "                        y * self.cell_size,\n",
    "                        self.cell_size,\n",
    "                        self.cell_size\n",
    "                    )\n",
    "                    pygame.draw.rect(overlay, cell_color, rect)\n",
    "\n",
    "        # Draw vision boundary circle (approximate)\n",
    "        center_pixel = (\n",
    "            int((center_x + 0.5) * self.cell_size),\n",
    "            int((center_y + 0.5) * self.cell_size)\n",
    "        )\n",
    "        radius_pixel = int((self.vision_radius + 0.5) * self.cell_size)\n",
    "\n",
    "        # Draw outer circle boundary\n",
    "        pygame.draw.circle(\n",
    "            overlay,\n",
    "            (*color, 100),\n",
    "            center_pixel,\n",
    "            radius_pixel,\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "        return overlay\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            return\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            pygame.font.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "\n",
    "            for agent_name in self.agents:\n",
    "               # self.agent_sprites[agent_name] = self._load_and_scale_sprite(f\"{agent_name}.png\", agent_name)\n",
    "               self.agent_sprites[agent_name] = self._load_and_scale_sprite(\".png\", agent_name)\n",
    "\n",
    "            for name, data in self.fixed_components.items():\n",
    "                self.component_sprites[name] = self._load_and_scale_sprite(data[\"file\"], name)\n",
    "\n",
    "\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((220, 220, 220))\n",
    "        pix_square_size = self.cell_size\n",
    "        sprite_offset = (pix_square_size - int(pix_square_size * 0.8)) // 2\n",
    "\n",
    "        # Render walls\n",
    "        for x in range(self.grid_size):\n",
    "            for y in range(self.grid_size):\n",
    "                if self.grid_map[x, y] == 1:\n",
    "                    pygame.draw.rect(\n",
    "                        canvas,\n",
    "                        (50, 50, 50),\n",
    "                        pygame.Rect(x * pix_square_size, y * pix_square_size, pix_square_size, pix_square_size)\n",
    "                    )\n",
    "\n",
    "        # Render vision overlays for each agent\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            vision_overlay = self._create_vision_overlay(self.agent_positions[agent], idx)\n",
    "            canvas.blit(vision_overlay, (0, 0))\n",
    "\n",
    "        # Render fixed components and agents\n",
    "        for name, data in self.fixed_components.items():\n",
    "            if self.gate_open and name == \"gate_close\":\n",
    "                continue\n",
    "            pos = data[\"pos\"]\n",
    "            x_coord = pos[0] * pix_square_size + sprite_offset\n",
    "            y_coord = pos[1] * pix_square_size + sprite_offset\n",
    "            sprite = self.component_sprites[name]\n",
    "            canvas.blit(sprite, (x_coord, y_coord))\n",
    "\n",
    "        for agent in self.agents:\n",
    "            pos = self.agent_positions[agent]\n",
    "            x_coord = pos[0] * pix_square_size + sprite_offset\n",
    "            y_coord = pos[1] * pix_square_size + sprite_offset\n",
    "\n",
    "            sprite = self.agent_sprites[agent]\n",
    "            canvas.blit(sprite, (x_coord, y_coord))\n",
    "\n",
    "        # Render the grid\n",
    "        for x in range(self.grid_size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas, 0, (0, pix_square_size * x), (self.window_size, pix_square_size * x), width=2\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas, 0, (pix_square_size * x, 0), (pix_square_size * x, self.window_size), width=2\n",
    "            )\n",
    "\n",
    "        # Display the render\n",
    "        self.window.blit(canvas, canvas.get_rect())\n",
    "        pygame.event.pump()\n",
    "        pygame.display.update()\n",
    "        self.clock.tick(4) # FPS\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95118daf",
   "metadata": {},
   "source": [
    "**Wrap the envornment in a PettingZoo env (for compatibility with Ray) and register it on Ray lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fd6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(config):\n",
    "    return ParallelPettingZooEnv(\n",
    "        MyGridWorld(render_mode=None, n_agents=4, vision_radius=2)\n",
    "    )\n",
    "\n",
    "register_env(\"my_gridworld\", env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cc4a4",
   "metadata": {},
   "source": [
    "## **IPPO - loose play settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0dc4f",
   "metadata": {},
   "source": [
    "Callback to register episode success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "class SuccessMetricCallback(DefaultCallbacks):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        \n",
    "        is_success = False\n",
    "        one_agent_id = list(episode.get_agents())[-1]\n",
    "        last_info = episode.last_info_for(one_agent_id)\n",
    "        is_success = last_info[\"is_success\"]\n",
    "       \n",
    "        episode.custom_metrics[\"success_rate\"] = int(is_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e0795",
   "metadata": {},
   "source": [
    "New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "dummy_env = MyGridWorld(n_agents=4)\n",
    "agent_ids = dummy_env.possible_agents\n",
    "\n",
    "multi_agent_policies = {\n",
    "    agent_id: (\n",
    "        None,  # default model\n",
    "        dummy_env.observation_spaces[agent_id],\n",
    "        dummy_env.action_spaces[agent_id],\n",
    "        {\n",
    "            \"model\": {\n",
    "                \"use_lstm\": True,\n",
    "                \"lstm_cell_size\": 128,\n",
    "                \"max_seq_len\": 100,\n",
    "                \"lstm_use_prev_action\": True,\n",
    "                \"lstm_use_prev_reward\": True,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    for agent_id in agent_ids\n",
    "}\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"my_gridworld\")\n",
    "    .env_runners(num_env_runners=0)\n",
    "    .callbacks(callbacks_class=SuccessMetricCallback)\n",
    "    .multi_agent(\n",
    "        policies=multi_agent_policies,\n",
    "        policy_mapping_fn=lambda agent_id, *args, **kwargs: agent_id\n",
    "    )\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_duration=10,\n",
    "        evaluation_duration_unit=\"episodes\",\n",
    "        evaluation_num_env_runners=1,\n",
    "        evaluation_config={\"explore\": True}\n",
    "    )\n",
    "    .training(\n",
    "        lr = 1e-4,\n",
    "        train_batch_size_per_learner=8000,\n",
    "        num_epochs=10,\n",
    "        entropy_coeff=0.01,\n",
    "        entropy_coeff_schedule = [\n",
    "            [0, 0.03],\n",
    "            [2e6, 0.005]\n",
    "        ],\n",
    "        kl_coeff=0.2,\n",
    "        kl_target=0.01,\n",
    "    )\n",
    ")\n",
    "\n",
    "dummy_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc5eb9",
   "metadata": {},
   "source": [
    "## **Save the agents model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efe300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_model(ppo, file_name):\n",
    "    current_dir = os.getcwd()\n",
    "    model_dir = os.path.join(current_dir, \"models\")\n",
    "    checkpoint_name = os.path.join(model_dir, file_name)\n",
    "    ppo.save(checkpoint_name)\n",
    "\n",
    "    print(f\"Model saved into {checkpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ad7bb",
   "metadata": {},
   "source": [
    "## **Wandb, 3 runs of training to generate esteme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "group_name = \"dense\"\n",
    "\n",
    "for run_idx in range(1,2):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"RUN NUMBER {run_idx}/3\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "\n",
    "    ppo = config.build_algo()\n",
    "    wandb.init(\n",
    "        project=\"gridworld_pomdp_looseplay_4agents_radius2\",\n",
    "        group=group_name,           \n",
    "        name=f\"run_{run_idx}\",    \n",
    "        reinit=True \n",
    "    )\n",
    "    file_name = f\"{under_examination}_run_{run_idx}\"\n",
    "    \n",
    "    num_total_episodes = 0\n",
    "    for i in range(200):\n",
    "        result = ppo.train()\n",
    "        num_total_episodes += result['env_runners']['num_episodes']\n",
    "        \n",
    "        print(f\"ITERATION {i}, num episode {num_total_episodes}\".center(100,\"-\"))\n",
    "        train_rew = round(result['env_runners']['episode_return_mean'], 2)\n",
    "        train_steps_episode = round(result['env_runners']['episode_len_mean'], 2)\n",
    "        train_success_rate = round(result['env_runners']['custom_metrics'].get('success_rate_mean', 0), 2) * 100\n",
    "        print(f\"\"\"TRAINING\\nRew:{train_rew} \\nSteps per episode:{train_steps_episode} \\nSuccess rate:{train_success_rate}%\\n\"\"\")\n",
    "\n",
    "        if 'evaluation' in result:\n",
    "            testing_rew = round(result['evaluation']['env_runners']['episode_return_mean'], 2)\n",
    "            testing_steps_episode = round(result['evaluation']['env_runners']['episode_len_mean'], 2)\n",
    "            testing_success_rate = round(result['evaluation']['env_runners']['custom_metrics'].get('success_rate_mean', 0), 2) * 100\n",
    "            print(f\"\"\"TESTING\\nRew:{testing_rew} \\nSteps per episode:{testing_steps_episode} \\nSuccess rate:{testing_success_rate}%\\n\\n\"\"\")\n",
    "\n",
    "        log_data = {\n",
    "            \"Episodes\": num_total_episodes,\n",
    "\n",
    "            \"Training_steps_per_episode\": train_steps_episode,\n",
    "            \"Training_success_rate_per_episode\": train_success_rate,\n",
    "            \"Training_reward_per_episode\": train_rew,\n",
    "\n",
    "            \"Testing_steps_per_episode\": testing_steps_episode,\n",
    "            \"Testing_success_rate_per_episode\": testing_success_rate,\n",
    "            \"Testing_reward_per_episode\": testing_rew\n",
    "        }\n",
    "        wandb.log(log_data)\n",
    "    save_model(ppo, file_name)\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3738be",
   "metadata": {},
   "source": [
    "## **Load agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2029a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO\n",
    "import os\n",
    "\n",
    "def load_model(file_name):\n",
    "    current_dir = os.getcwd()\n",
    "    model_dir = os.path.join(current_dir, \"models\")\n",
    "    checkpoint_name = os.path.join(model_dir, file_name)\n",
    "\n",
    "    ppo = PPO.from_checkpoint(checkpoint_name)\n",
    "    return ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59369442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:527: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "2026-01-07 01:42:33,827\tINFO worker.py:2007 -- Started a local Ray instance.\n",
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "2026-01-07 01:42:35,896\tWARNING recurrent_net.py:85 -- DeprecationWarning: `ray.rllib.models.torch.recurrent_net.RecurrentNetwork` has been deprecated. This will raise an error in the future!\n",
      "[2026-01-07 01:42:36,008 E 839117 839117] core_worker.cc:2223: Actor with class name: 'RolloutWorker' and ID: '5569b787eb4dbe5d0edcf6c501000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(RolloutWorker pid=839352)\u001b[0m /home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\u001b[36m(RolloutWorker pid=839352)\u001b[0m   from pkg_resources import resource_stream, resource_exists\n",
      "\u001b[36m(RolloutWorker pid=839352)\u001b[0m DeprecationWarning: `ray.rllib.models.torch.recurrent_net.RecurrentNetwork` has been deprecated. This will raise an error in the future!\n",
      "2026-01-07 01:42:39,276\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "model_dir = os.path.join(current_dir, \"models\")\n",
    "\n",
    "for filename in os.listdir(model_dir):\n",
    "    if under_examination in filename:\n",
    "        matching_file = filename\n",
    "        break\n",
    "\n",
    "file_path = os.path.join(model_dir, matching_file)\n",
    "ppo = load_model(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c4cf2",
   "metadata": {},
   "source": [
    "## **Graphically test the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28da8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Sprite image /home/terra/Desktop/magistrale/distributed_ai_project/sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image /home/terra/Desktop/magistrale/distributed_ai_project/sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image /home/terra/Desktop/magistrale/distributed_ai_project/sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image /home/terra/Desktop/magistrale/distributed_ai_project/sprites/.png not found. Using default sprite for this simulation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-07 01:43:02,941 E 839178 839178] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2026-01-07 01:43:03,756 E 839292 839292] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Out of time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=839352)\u001b[0m [2026-01-07 01:43:04,695 E 839352 839437] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2026-01-07 01:43:04,945 E 839117 839345] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ Out of time\n",
      "✓ Success\n",
      "\n",
      "==================================================\n",
      "Success rate: 1/3 = 33.3%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "env = MyGridWorld(grid_size=15, n_agents=4, render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "lstm_size = 128\n",
    "\n",
    "# Init LSTM states: (h, c) for each agent\n",
    "lstm_states = {\n",
    "    agent_id: [torch.zeros(1, lstm_size), torch.zeros(1, lstm_size)] \n",
    "    for agent_id in env.possible_agents\n",
    "}\n",
    "\n",
    "# Track previous action/reward for each agent\n",
    "prev_actions = {agent_id: 0 for agent_id in env.possible_agents}\n",
    "prev_rewards = {agent_id: 0.0 for agent_id in env.possible_agents}\n",
    "\n",
    "num_episodes = 0\n",
    "successes = 0\n",
    "\n",
    "while num_episodes < 3:\n",
    "    actions = {}\n",
    "    \n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        # Prepare batched inputs (batch_size=1)\n",
    "        obs_batch = np.array([agent_obs], dtype=np.float32)\n",
    "        prev_action_batch = np.array([prev_actions[agent_id]], dtype=np.float32)\n",
    "        prev_reward_batch = np.array([prev_rewards[agent_id]], dtype=np.float32)\n",
    "        \n",
    "        # Compute action (RLlib handles sequence buffering internally via LSTM states)\n",
    "        action, new_state, _ = ppo.get_policy(agent_id).compute_actions(\n",
    "            obs_batch=obs_batch,\n",
    "            state_batches=lstm_states[agent_id],\n",
    "            prev_action_batch=prev_action_batch,\n",
    "            prev_reward_batch=prev_reward_batch,\n",
    "            explore=True\n",
    "        )\n",
    "        \n",
    "        actions[agent_id] = int(action[0])\n",
    "        lstm_states[agent_id] = new_state  # Update LSTM state\n",
    "    \n",
    "    # Step environment\n",
    "    obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    \n",
    "    # Update prev_actions and prev_rewards\n",
    "    for agent_id in obs.keys():\n",
    "        prev_actions[agent_id] = actions[agent_id]\n",
    "        prev_rewards[agent_id] = rewards[agent_id]\n",
    "    \n",
    "    # Check episode end\n",
    "    if any(terminations.values()) or any(truncations.values()):\n",
    "        if any(terminations.values()):\n",
    "            print(\"✓ Success\")\n",
    "            successes += 1\n",
    "        else:\n",
    "            print(\"✗ Out of time\")\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Reset LSTM states at episode start\n",
    "        lstm_states = {\n",
    "            agent_id: [torch.zeros(1, lstm_size), torch.zeros(1, lstm_size)]\n",
    "            for agent_id in env.possible_agents\n",
    "        }\n",
    "        prev_actions = {agent_id: 0 for agent_id in env.possible_agents}\n",
    "        prev_rewards = {agent_id: 0.0 for agent_id in env.possible_agents}\n",
    "        \n",
    "        num_episodes += 1\n",
    "\n",
    "env.close()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Success rate: {successes}/{num_episodes} = {successes/num_episodes*100:.1f}%\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
